{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auditing_Data_Quality\n",
    "\n",
    "## Measuring Data Quality\n",
    "\n",
    "1. Validity:  \n",
    "Measuring the degree entries conform the schema and other constrains\n",
    "\n",
    "2. Accuracy: \n",
    "Compare entries to Gold standard data (reference data)\n",
    "\n",
    "3. Completeness: Do we have all the data we need\n",
    "we look for the missing records. To fix them we need reference data to compare\n",
    "\n",
    "4. Consistency:\n",
    "When two entries contradict with each other. Then we should find which data source we trust most. Or which data is newer. Or by which instrument is more accurate. \n",
    "\n",
    "5. Uniformity\n",
    "Then means that all the fields are using the same units of measuments.\n",
    "\n",
    "## Process to cleaning the data:\n",
    "\n",
    "1. Audit the data \n",
    " Programitaclly checking the data using validation rules  and creat report on the quality of the data. We can run statistical analysis to check for outlayers. \n",
    "\n",
    "2. Creat data cleaning plan.\n",
    "We need to find causes of strange data that we are seeing. Define a set of operations that we correct our data. and at the end we run some tests. \n",
    "\n",
    "3. Excute the plan\n",
    "\n",
    "4. Manually correct the data\n",
    "\n",
    "5. Start from step one and do over. \n",
    "\n",
    "\n",
    "#  Corss-Field Constraint\n",
    "When we have multiple fields per item, that must be in agremment in some way.\n",
    "\n",
    "\n",
    "\n",
    "Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project : Correcting Validity\n",
    "The task is to check the \"productionStartYear\" of the DBPedia autos datafile for valid values.\n",
    "The following things should be done:\n",
    "- check if the field \"productionStartYear\" contains a year\n",
    "- check if the year is in range 1886-2014\n",
    "- convert the value of the field to be just a year (not full datetime)\n",
    "- the rest of the fields and values should stay the same\n",
    "- if the value of the field is a valid year in the range as described above,\n",
    "  write that line to the output_good file\n",
    "- if the value of the field is not a valid year as described above, \n",
    "  write that line to the output_bad file\n",
    "- discard rows (neither write to good nor bad) if the URI is not from dbpedia.org\n",
    "- you should use the provided way of reading and writing data (DictReader and DictWriter)\n",
    "  They will take care of dealing with the header.\n",
    "\n",
    "You can write helper functions for checking the data and writing the files, but we will call only the \n",
    "'process_file' with 3 arguments (inputfile, output_good, output_bad).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import csv\n",
    "import pprint\n",
    "\n",
    "INPUT_FILE = 'data/autos1.csv'\n",
    "OUTPUT_GOOD = 'data/autos-valid.csv'\n",
    "OUTPUT_BAD = 'data/autos-Fixme.csv'\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "def process_file(input_file, output_good, output_bad):\n",
    "    # store data into lists for output\n",
    "    data_good = []\n",
    "    data_bad = []\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        header = reader.fieldnames\n",
    "        for row in reader:\n",
    "            # validate URI value\n",
    "            if row['URI'].find(\"dbpedia.org\") < 0:\n",
    "                continue\n",
    "\n",
    "            ps_year = row['productionStartYear'][:4]\n",
    "            try: # use try/except to filter valid items\n",
    "                ps_year = int(ps_year)\n",
    "                row['productionStartYear'] = ps_year\n",
    "                if (ps_year >= 1886) and (ps_year <= 2014):\n",
    "                    data_good.append(row)\n",
    "                else:\n",
    "                    data_bad.append(row)\n",
    "            except ValueError: # non-numeric strings caught by exception\n",
    "                if ps_year == 'NULL':\n",
    "                    data_bad.append(row)\n",
    "\n",
    "    # Write processed data to output files\n",
    "    with open(output_good, \"w\", encoding=\"utf-8\") as good:\n",
    "        writer = csv.DictWriter(good, delimiter=\",\", fieldnames= header)\n",
    "        writer.writeheader()\n",
    "        for row in data_good:\n",
    "            writer.writerow(row)\n",
    "\n",
    "    with open(output_bad, \"w\", encoding=\"utf-8\") as bad:\n",
    "        writer = csv.DictWriter(bad, delimiter=\",\", fieldnames= header)\n",
    "        writer.writeheader()\n",
    "        for row in data_bad:\n",
    "            writer.writerow(row)\n",
    "            \n",
    "\n",
    "def test():\n",
    "\n",
    "    process_file(INPUT_FILE, OUTPUT_GOOD, OUTPUT_BAD)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project : \n",
    "\n",
    "### Check data quality\n",
    "\n",
    "In this problem set we work with cities infobox data, audit it, come up with a cleaning idea and then clean it up.\n",
    "In the first exercise we want you to audit the datatypes that can be found in some particular fields in the dataset.\n",
    "The possible types of values can be:\n",
    "- 'NoneType' if the value is a string \"NULL\" or an empty string \"\"\n",
    "- 'list', if the value starts with \"{\"\n",
    "- 'int', if the value can be cast to int\n",
    "- 'float', if the value can be cast to float, but is not an int\n",
    "- 'str', for all other values\n",
    "\n",
    "The audit_file function should return a dictionary containing fieldnames and the datatypes that can be found in the field.\n",
    "All the data initially is a string, so you have to do some checks on the values first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'areaCode': {<class 'int'>, <class 'NoneType'>, <class 'str'>},\n",
      " 'areaLand': {<class 'list'>, <class 'NoneType'>, <class 'float'>},\n",
      " 'areaMetro': {<class 'NoneType'>, <class 'float'>},\n",
      " 'areaUrban': {<class 'NoneType'>, <class 'float'>},\n",
      " 'elevation': {<class 'list'>, <class 'NoneType'>, <class 'float'>},\n",
      " 'governmentType_label': {<class 'NoneType'>, <class 'str'>},\n",
      " 'homepage': {<class 'NoneType'>, <class 'str'>},\n",
      " 'isPartOf_label': {<class 'list'>, <class 'str'>, <class 'NoneType'>},\n",
      " 'maximumElevation': {<class 'NoneType'>},\n",
      " 'minimumElevation': {<class 'NoneType'>},\n",
      " 'name': {<class 'list'>, <class 'str'>, <class 'NoneType'>},\n",
      " 'populationDensity': {<class 'list'>, <class 'NoneType'>, <class 'float'>},\n",
      " 'populationTotal': {<class 'int'>, <class 'NoneType'>},\n",
      " 'timeZone_label': {<class 'str'>, <class 'NoneType'>},\n",
      " 'utcOffset': {<class 'list'>,\n",
      "               <class 'int'>,\n",
      "               <class 'NoneType'>,\n",
      "               <class 'str'>},\n",
      " 'wgs84_pos#lat': {<class 'float'>},\n",
      " 'wgs84_pos#long': {<class 'float'>}}\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import csv\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "CITIES = 'data/cities.csv'\n",
    "\n",
    "FIELDS = [\"name\", \"timeZone_label\", \"utcOffset\", \"homepage\", \"governmentType_label\", \"isPartOf_label\", \"areaCode\", \"populationTotal\", \n",
    "          \"elevation\", \"maximumElevation\", \"minimumElevation\", \"populationDensity\", \"wgs84_pos#lat\", \"wgs84_pos#long\", \n",
    "          \"areaLand\", \"areaMetro\", \"areaUrban\"]\n",
    "\n",
    "def is_int(value):\n",
    "    try:\n",
    "        int(value)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def is_float(value):\n",
    "    try:\n",
    "        float(value)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def audit_file(filename, fields):\n",
    "    fieldtypes = {}\n",
    "\n",
    "    for field in fields:\n",
    "        fieldtypes[field] = set()\n",
    "\n",
    "    with open(filename, 'r') as input:\n",
    "        reader = csv.DictReader(input)\n",
    "\n",
    "        for row in reader:\n",
    "            if row['URI'].find('dbpedia') > -1:\n",
    "                \n",
    "                for field in fields:\n",
    "\n",
    "                    if row[field] == \"NULL\" or row[field] == \"\":\n",
    "                        fieldtypes[field].add(type(None))\n",
    "                    elif row[field][0] == \"{\":\n",
    "                        fieldtypes[field].add(type([]))\n",
    "                    elif is_int(row[field]):\n",
    "                        fieldtypes[field].add(type(1))    \n",
    "                    elif is_float(row[field]):\n",
    "                        fieldtypes[field].add(type(1.1))\n",
    "                    else:\n",
    "                        fieldtypes[field].add(type(''))  \n",
    "\n",
    "    return fieldtypes\n",
    "\n",
    "\n",
    "def test():\n",
    "    fieldtypes = audit_file(CITIES, FIELDS)\n",
    "\n",
    "    pprint.pprint(fieldtypes)\n",
    "    \n",
    "    assert fieldtypes[\"areaLand\"] == set([type(1.1), type([]), type(None)])\n",
    "    assert fieldtypes['areaMetro'] == set([type(1.1), type(None)])\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing the Area\n",
    "In this part we work with the cities infobox data. It will receive a string as an input, and it has to return a float representing the value of the area or None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing three example results:\n",
      "None\n",
      "101900000.0\n",
      "31700000.0\n",
      "55300000.0\n",
      "63713700.0\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import csv\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "CITIES = 'data/cities.csv'\n",
    "\n",
    "\n",
    "def fix_area(area):\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    try:\n",
    "        area= float(area)\n",
    "    except ValueError:\n",
    "        if area[0] == \"{\":\n",
    "            area = area.replace(\"{\", \"\").replace(\"}\", \"\").split(\"|\")\n",
    "            return float(max(area[0],area[1]))        \n",
    "        return None\n",
    "    return area\n",
    "\n",
    "\n",
    "def process_file(filename):\n",
    "    # CHANGES TO THIS FUNCTION WILL BE IGNORED WHEN YOU SUBMIT THE EXERCISE\n",
    "    data = []\n",
    "\n",
    "    with open(filename, \"r\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "\n",
    "        #skipping the extra metadata\n",
    "        for i in range(3):\n",
    "            l = next(reader)\n",
    "\n",
    "        # processing file\n",
    "        for line in reader:\n",
    "            # calling your function to fix the area value\n",
    "            if \"areaLand\" in line:\n",
    "                line[\"areaLand\"] = fix_area(line[\"areaLand\"])\n",
    "            data.append(line)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def test():\n",
    "    data = process_file(CITIES)\n",
    "\n",
    "    print('Printing three example results:')\n",
    "    for n in range(5,10):\n",
    "        pprint.pprint(data[n][\"areaLand\"])\n",
    "\n",
    "    assert data[3][\"areaLand\"] == None        \n",
    "    assert data[7][\"areaLand\"] == 31700000.0\n",
    "    assert data[19][\"areaLand\"] == 4480000.0\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing the Name\n",
    "We can see that the \"name\" value can be an array (or list in Python terms). It would make it easier to process and query the data later if all values for the name are in a Python list, instead of being just a string separated with special characters, like now.\n",
    "\n",
    "\n",
    "we use  the function fix_name(). It will recieve a string as an input, and it\n",
    "will return a list of all the names. If there is only one name, the list will\n",
    "have only one item in it; if the name is \"NULL\", the list should be empty.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "txt = \"Ali reza\"\n",
    "y = txt.split(\" \")\n",
    "print(type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing 20 results:\n",
      "['Kud']\n",
      "['Kuju']\n",
      "['Kumbhraj']\n",
      "['Kumhari']\n",
      "['Kunigal']\n",
      "['Kurgunta']\n",
      "['Athens']\n",
      "['Demopolis']\n",
      "['Chelsea Alabama']\n",
      "['Pell City Alabama']\n",
      "['City of Northport']\n",
      "['Sand Point']\n",
      "['Unalaska Alaska']\n",
      "['City of Menlo Park']\n",
      "['Negtemiut', 'Nightmute']\n",
      "['Fairbanks Alaska']\n",
      "['Homer']\n",
      "['Ketchikan Alaska']\n",
      "['Nuniaq', 'Old Harbor']\n",
      "['Rainier Washington']\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import csv\n",
    "import pprint\n",
    "\n",
    "CITIES = 'data/cities.csv'\n",
    "\n",
    "\n",
    "def fix_name(name):\n",
    "\n",
    "    if name == \"NULL\" or name == \"\":\n",
    "        return []\n",
    "    else:\n",
    "        return name.replace('{', '').replace('}','').split('|')\n",
    "\n",
    "    return name\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_file(filename):\n",
    "    data = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        #skipping the extra metadata\n",
    "        for i in range(3):\n",
    "            l = next(reader)\n",
    "            \n",
    "        # processing file\n",
    "        for line in reader:\n",
    "            # calling your function to fix the area value\n",
    "            if \"name\" in line:\n",
    "                line[\"name\"] = fix_name(line[\"name\"])\n",
    "            data.append(line)\n",
    "    return data\n",
    "\n",
    "\n",
    "def test():\n",
    "    data = process_file(CITIES)\n",
    "\n",
    "    print(\"Printing 20 results:\")\n",
    "    for n in range(20):\n",
    "        pprint.pprint(data[n][\"name\"])\n",
    "\n",
    "    assert data[14][\"name\"] == ['Negtemiut', 'Nightmute']\n",
    "    assert data[9][\"name\"] == ['Pell City Alabama']\n",
    "    assert data[3][\"name\"] == ['Kumhari']\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
